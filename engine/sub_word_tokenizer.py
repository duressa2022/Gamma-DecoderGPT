from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace
import pickle
import numpy as np

class SubWordTokenizer:
    def __init__(self,number_token):
        #create tokenizer here
        self.tokenizer=Tokenizer(WordPiece(unk_token="[UNK]"))
        #pre tokenize based on whitespace 
        self.tokenizer.pre_tokenizer=Whitespace()
        #create a trainer model 
        self.trainer=WordPieceTrainer(
            vocab_size=number_token,
            special_tokens=["[PAD]","[UNK]","[SEP]","[CLS]"]
        )
        #define special tokens 
        self.specials=["[PAD]","[CLS]","[SEP]","[START]","[END]","[UNK]"]
    def tokenize(self,input,save_path):
        #train the tokenizer 
        self.tokenizer.train_from_iterator(input,trainer=self.trainer)
        #check wether the path is found
        if save_path:
           #open the file as write mode
           with open(file=save_path,mode="wb") as file:
               #write binary into the file 
               pickle.dump(self.tokenizer,file=file)
        #return the tokenizer 
        return self.tokenizer.get_vocab()
    
    def load(self,saved_path):
        #check wether the path is found
        with open(file=saved_path,mode="rb") as file:
            #load the pickle file 
            self.tokenizer=pickle.load(file)
        #return result
        return self.tokenizer.get_vocab()
    def encode(self,inputs):
        #encode the senctence by using trained tokenizer 
        #the enocoded sentence have:token and token id in vocab
        return [self.tokenizer.encode(input) for input in inputs]
    def decode(self,ids):
        #decode the ids by using trained tokenizer 
        #decode one os token words;tokenzed
        return self.tokenizer.decode(ids=ids)
    def vectorize(self,e_sequences,token_vocab,max_length):
        #e_sequenece: generated by using encode(e_sequenecs)
        #token_vocab: vocabulary of tokens 
        #max_length: max_length index need
        #get padding index from vocab
        padding_index=token_vocab["[PAD]"]
        #define result list 
        result=[]
        #iterate through the encoder sequence 
        for e in e_sequences:
            #get index for current sequence
            indices=e.ids
            #make the length of indices max_length 
            indices=indices[:max_length]
            #current length<max_length add padding 
            indices=indices+[padding_index]*(max_length-len(indices))
            #append to the result list 
            result.append(indices)
        #conv to numpy array and return 
        return np.array(result)
    def build_token(self,indices,vocabs):
        #create a index:token mapping
        mapping={index:token for token,index in vocabs.items()}
        #create alist of token from indices and return
        return [mapping[index] for index in indices]
    def build_sentence(self,tokens):
        #remove special token from the list
        tokens=[token for token in tokens if token not in self.specials]
        #define result list 
        result=[]
        #iterate through the token 
        for token in tokens:
            #check wether the token starts with "##"
            if token.startswith("##"):
                #check if there is prev token 
                if result[-1]:
                    #combine these two tokens
                    result[-1]+=token.replace("##","")
            else:
                #or append the token to result
                result.append(token)
        #join tokens and return 
        return " ".join(result).strip()
app=SubWordTokenizer(number_token=1000)
inputs=[
    "this is your work",
    "why dont you work ",
    "start doing the job"
]

# v=app.tokenize(inputs,"C:\\Users\\HP\\Documents\\learningpath\\pytorch\\gamma-decoderGPT\\__app_model\\word.pkl")
# print(v)
# vocabs=app.load("C:\\Users\\HP\\Documents\\learningpath\\pytorch\\gamma-decoderGPT\\__app_model\\word.pkl")
# # print(vocabs)
# test=["your name is what"]
# e=app.encode(test)
# d=app.decode(e[0].ids)
# v=app.vectorize(e,vocabs,10)
# t=app.build_token(v[0],vocabs)
# s=app.build_sentence(t)
# print(s)




    






    
